---
layout: about
title: Home
permalink: /
subtitle: <strong><font size="4">AI/ML Researcher</font></strong>

profile:
  align: right
  image: prof_pic.jpg
  image_circular: false # crops the image to make it circular
  more_info:  # > 
  # ðŸ¤– Human-centered AI, ðŸ¤ Alignment * ðŸ•µ Interpretability 
  # >
    # <a href="https://scholar.google.com/citations?user=NjhpUykAAAAJ">Google Scholar</a>, <a href="https://github.com/aounon">GitHub</a>, <a href="https://www.linkedin.com/in/aounon-kumar/">LinkedIn</a>

news: false # includes a list of news items
latest_posts: false # includes a list of the newest posts
selected_papers: false # includes a list of papers marked as "selected={true}"
social: false # includes social icons at the bottom of the page
---

I am a software engineer at Personal Robots group of MIT Media Lab. My long term research endeavor is to build aritificial integlligence (AI) systems that support humans decision making under uncertainty. Currently, my research revolves around `interpretability` and `alignment` of language models in educational settings. 

<!-- It involves designing algorithms to defend models against adversarial inputs, for example, safeguarding large language models (LLMs) from prompts that circumvent safety guardrails. I have studied and contributed to model robustness in several machine learning domains including `computer vision`, `reinforcement learning`, and `language modeling`. My work has been accepted in prominent ML conferences such as `ICML`, `ICLR` and `NeurIPS`, and I am actively involved in collaborative projects within the academic community. -->

**Media Coverage:** My recent works on `LLM robustness` and `reliability` have been featured in major media and academic news outlets:
  1. [Science News Magazine](https://www.sciencenews.org/article/generative-ai-chatbots-chatgpt-safety-concerns?trk=feed_main-feed-card_feed-article-content), [D^3 Institute at Harvard](https://d3.harvard.edu/certifying-llm-safety-against-adversarial-prompting/). Work featured: [Certifying LLM Safety against Adversarial Prompting](https://openreview.net/forum?id=9Ik05cycLq).
  2. [The New York Times](https://www.nytimes.com/2024/08/30/technology/ai-chatbot-chatgpt-manipulation.html). Work featured: [Manipulating Large Language Models to Increase Product Visibility](https://arxiv.org/abs/2404.07981).
  3. [The Washington Post](https://www.washingtonpost.com/technology/2023/06/02/turnitin-ai-cheating-detector-accuracy/), [Bloomberg](https://www.bloomberg.com/news/newsletters/2023-11-06/biden-ai-executive-order-shows-urgency-of-deepfakes), [Wired](https://www.wired.com/story/ai-detection-chat-gpt-college-students/), [New Scientist](https://www.newscientist.com/article/2366824-reliably-detecting-ai-generated-text-is-mathematically-impossible/), [The Register](https://www.theregister.com/2023/03/21/detecting_ai_generated_text/), [TechSpot](https://www.techspot.com/news/98031-reliable-detection-ai-generated-text-impossible-new-study.html). Work featured: [Can AI-Generated Text be Reliably Detected?](https://arxiv.org/abs/2303.11156).

Before joining Harvard, I completed my PhD at the [University of Maryland](https://www.cs.umd.edu) in `certified robustness` in machine learning (see my dissertation [here](https://drum.lib.umd.edu/items/f4ad78d5-f6a8-47cf-bdca-358410186a96)). I was fortunate to be advised by Professors [Soheil Feizi](https://www.cs.umd.edu/~sfeizi/) and [Tom Goldstein](https://www.cs.umd.edu/~tomg/). During my PhD, I have spent time as an applied scientist intern at [Amazon](https://www.amazon.science/) and a research intern at [Nokia Bell Labs](https://www.bell-labs.com/), where I worked on uncertainty estimation for human action recognition models and network security-related machine learning applications. I have also served as a reviewer for machine learning conferences such as ICML, ICLR and NeurIPS.

I did my undergraduate studies at [IIT Mandi](https://iitmandi.ac.in/) and my master's at [IIT Delhi](https://home.iitd.ac.in/), where I studied a wide range of topics in computer science such as machine learning, advanced algorithms, combinatorial optimization, complexity theory and cryptography. My master's thesis was on the computational hardness of approximating the optimal solution of a variant of the k-center clustering problem.

<br>

<!-- ## News

| | |
| **Aug 30, 2024** &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; | :page_facing_up: New pre-print on [Manipulating Large Language Models to Increase Product Visibility](https://arxiv.org/abs/2404.07981)! Covered by [The New York Times](https://www.nytimes.com/2024/08/30/technology/ai-chatbot-chatgpt-manipulation.html). |
|  |  |
| **Jul 10, 2024** &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; | Accepted at COLM 2024 :llama:: [Certifying LLM Safety against Adversarial Prompting](https://openreview.net/forum?id=9Ik05cycLq). Covered by [Science News Magazine](https://www.sciencenews.org/article/generative-ai-chatbots-chatgpt-safety-concerns?trk=feed_main-feed-card_feed-article-content). |
|  |  |
| **Dec 19, 2023** | Graduated from `UMD`! :man_student: | -->

<!-- table>
  <tr><td><br></td><td><br></td></tr>
  <tr>
    <td valign="top" width="20%"><strong>Feb 12, 2024</strong></td>
    <td> New pre-print on <a href="https://arxiv.org/abs/2309.02705">Certifying LLM Safety against Adversarial Prompting</a>. Covered by <a href="https://www.sciencenews.org/article/generative-ai-chatbots-chatgpt-safety-concerns?trk=feed_main-feed-card_feed-article-content">Science News Magazine</a>.
    </td>
  </tr>
  <tr><td><br></td><td><br></td></tr>
  <tr>
    <td valign="top" width="20%"><strong>Dec 19, 2023</strong></td>
    <td>Graduated from `UMD`! :man_student:</td>
  </tr>
  <tr><td><br></td><td><br></td></tr>
  <tr>
    <td valign="top" width="20%"><strong>Oct 05, 2023</strong></td>
    <td>Started as a Research Associate at `Harvard University`.</td>
  </tr>
</table -->

## Publications
<!-- See full list at [Google Scholar](https://scholar.google.com/citations?user=NjhpUykAAAAJ). -->

<table>
  <tr><td><br></td><td><br></td></tr>
  <tr>
    <td valign="top" width="20%"><strong>EMNLP 2025 Findings</strong></td>
    <td><strong>BehaviorSFT: Behavioral Token Conditioning for Clinical Agents Across the
Proactivity Spectrum</strong><br>
        Yubin Kim, Zhiyuan Hu, Hyewon Jeong, <b>Eugene Park</b>, Shuyue Stella Li, Chanwoo Park, Shiyun
Xiong, MingYu Lu, Hyeonhoon Lee, Xin Liu, Daniel McDuff, Cynthia Breazeal, Samir Tulebaev,
Hae Won Park<br>
        <a href="https://arxiv.org/abs/2505.21757">ArXiv</a>, <a href="https://behavior-adaptation.github.io/">Code & Model</a>
    </td>
  </tr>

    <tr><td><br></td><td><br></td></tr>
  <tr>
    <td valign="top" width="20%"><strong>Interspeech 2025 Oral</strong></td>
    <td><strong>VocalAgent: Large Language
Models for Vocal Health Diagnostics with Safety-Aware Evaluation</strong><br>
        Yubin Kim, Taehan Kim, Wonjune Kang, <b>Eugene Park</b>,, Joonsik Yoon, Dongjae Lee, Xin Liu,
Daniel McDuff, Hyeonhoon Lee, Cynthia Breazeal, Hae Won Park<br>
        <a href="https://arxiv.org/abs/2505.13577">ArXiv</a>
    </td>
  </tr>
  
  <tr><td><br></td><td><br></td></tr>
  <tr>
    <td valign="top" width="20%"><strong>Preprint 2025</strong></td>
    <td><strong>Medical Hallucination in Foundation Models and Their Impact on Healthcare</strong><br>
        Yubin Kim, Hyewon Jeong, Shan Chen, Shuyue Stella Li, Mingyu Lu, Kumail Alhamoud, Jimin
Mun, Cristina Grau, Minseok Jung, Rodrigo Gameiro, Lizhou Fan, <b>Eugene Park</b>, Tristan Lin,
Joonsik Yoon, Wonjin Yoon, Maarten Sap, Yulia Tsvetkov, Paul Liang, Xuhai Xu, Xin Liu, Daniel
McDuff, Hyeonhoon Lee, Hae Won Park, Samir Tulebaev, Cynthia Breazeal
        <br>
        <a href="https://www.medrxiv.org/content/10.1101/2025.02.28.25323115v1.full">medRxiv</a>
      </td>
  </tr>
  <tr><td><br></td><td><br></td></tr>

  <tr><td><br></td><td><br></td></tr>
  <tr>
    <td valign="top" width="20%"><strong>Preprint 2024</strong></td>
    <td><strong>Swamps, Springboards,  and the Demographic Dynamics  of Occupational Mobility Identifying Enablers and Inhibitors of Career Growth by Mining Education and Employment Histories  for Millions of U.S. Workers</strong><br>
        Chris Compton, <b>Eugene Park</b>, Matthew Walsh , George Westerman<br>
        <a href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=5616190">SSRN</a>
    </td>
  </tr>
  <tr><td><br></td><td><br></td></tr>
  <tr>
    <td valign="top" width="20%"><strong>Preprint 2024</strong></td>
    <td><strong>Real-World Pathways to Manufacturing Jobs</strong><br>
        Jeff Dieffenbach, <b>Eugene Park</b>, George Westerman<br>
        <a href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=5619230">SSRN</a>
    </td>
  </tr>
  <tr><td><br></td><td><br></td></tr>
  <tr>
    <td valign="top" width="20%"><strong>Preprint 2023</strong></td>
    <td><strong>Principal Component Analysis and Hidden Markov Model for Forecasting Stock Returns</strong><br>
        <b>Eugene Park</b><br>
        <a href="https://arxiv.org/abs/2307.00459">ArXiv</a>
    </td>
  </tr>
</table>

<br>

## Contact

[Science and Engineering Complex](https://seas.harvard.edu/about-us/visit-us/allston/science-engineering-complex) :office:<br>
150 Western Ave<br>
Office #6220<br>
Allston, MA 02134

<!-- Write your biography here. Tell the world about yourself. Link to your favorite [subreddit](http://reddit.com). You can put a picture in, too. The code is already in, just name your picture `prof_pic.jpg` and put it in the `img/` folder.

Put your address / P.O. box / other info right below your picture. You can also disable any of these elements by editing `profile` property of the YAML header of your `_pages/about.md`. Edit `_bibliography/papers.bib` and Jekyll will render your [publications page](/al-folio/publications/) automatically.

Link to your social media connections, too. This theme is set up to use [Font Awesome icons](https://fontawesome.com/) and [Academicons](https://jpswalsh.github.io/academicons/), like the ones below. Add your Facebook, Twitter, LinkedIn, Google Scholar, or just disable all of them. -->
